{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07ce67a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N1, N2, N3, N4, N9, NA, VPr\n",
      "V1, V2, V3, V4, V5, VPr\n",
      "C0, C1\n",
      "Po, N9, Pr, Pr1\n",
      "A0, A1, A2, \n",
      "nan\n",
      "Ad\n",
      "No, Nu\n",
      "Sp, NA\n",
      "N5, N6, N7, N8, NA\n"
     ]
    }
   ],
   "source": [
    "import lib.get_assets as assets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "876bc475",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lib.model0_9 as m0_9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ee6567",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "74dd4ae5",
   "metadata": {},
   "source": [
    "### Rule 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc27b416",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tests = [{'farsi': 'ییلاق‌نشین', 'trans': 'yeylAqneSin'},\n",
    "         {'farsi': 'ییدیش', 'trans': 'yidiS'},\n",
    "         {'farsi': 'یوونتوس', 'trans': 'yuventus'},\n",
    "         {'farsi': 'یونید', 'trans': 'یونید'} # case verb\n",
    "        ]\n",
    "\n",
    "for t in tests:   \n",
    "    assert m0_9.general_search(t['farsi'], pos_neg='Verb') == t['trans']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf71ac43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c8ea2fb4",
   "metadata": {},
   "source": [
    "### Rule 2 (Nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e70bac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tests = [{'farsi': 'پیامبرت', 'trans': 'payAmbarat'},\n",
    "         {'farsi': 'مزايایی', 'trans': 'mazAyAyi'},\n",
    "         {'farsi': 'بی شرف', 'trans': 'bi Saraf'},\n",
    "         {'farsi': 'بی عقل', 'trans':'bi \\'aql'}]\n",
    "\n",
    "for t in tests:\n",
    "    assert m0_9.run_transcription_0(t['farsi']) == t['trans']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8708fd9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a309af8a",
   "metadata": {},
   "source": [
    "### Rule 3 (Verbs basis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4131717c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tests = [{'farsi': 'بخواهند بروند', 'trans': 'bexAhand beravand'},\n",
    "         # {'farsi': 'می‌تواند بخوابد', 'trans': 'mitavAnad bexAbad'}, # \\u200c\n",
    "         # {'farsi': 'نشد بپریم', 'trans': 'naSod beparim'}, # می\n",
    "         # {'farsi': 'می روید', 'trans': 'miravid'} # \n",
    "        # {'farsi': 'می ترسد', 'trans': 'mitarsad'}\n",
    "        ]\n",
    "\n",
    "for t in tests:\n",
    "    assert m0_9.run_transcription_0(t['farsi']) == t['trans']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc2caa7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "516a75a9",
   "metadata": {},
   "source": [
    "### Rule 4 (Frequency based prioritization for collisions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "744a265a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tests = [{'farsi': 'ی', 'trans': 'ye'}]\n",
    "\n",
    "for t in tests:\n",
    "    assert m0_9.general_search(t['farsi']) == t['trans']\n",
    "\n",
    "    \n",
    "tests = [{'farsi': 'ی', 'trans': 'i'}]\n",
    "for t in tests:\n",
    "    assert m0_9.affix_search(t['farsi']) == t['trans']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0b3333",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "434c50be",
   "metadata": {},
   "source": [
    "### Rule 5 (Verb _ as خواهند_كر)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7d40975",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tests = [{'farsi': 'خواهند_كرد', 'trans': 'xAhand kard'}]\n",
    "\n",
    "for t in tests:\n",
    "    assert m0_9.process_verb(m0_9.normalise(t['farsi'])) == t['trans']\n",
    "\n",
    "tests = [{'farsi': 'خواهند كرد', 'trans': 'xAhand kard'},\n",
    "         {'farsi': 'بخواهند بروند', 'trans': 'bexAhand beravand'}]\n",
    "\n",
    "for t in tests:\n",
    "    assert m0_9.run_transcription_0(t['farsi']) == t['trans']\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7adfb46d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "705bc115",
   "metadata": {},
   "source": [
    "### Rule 6 (normalisation ي, ك)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "abafaa98",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tests = [{'farsi': 'گزارشي', 'trans': 'گزارشی'},\n",
    "         {'farsi': 'ك', 'trans': 'ک'}]\n",
    "\n",
    "for t in tests:\n",
    "    assert m0_9.normalise(t['farsi']) == t['trans']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9dddcdd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7df932ee",
   "metadata": {},
   "source": [
    "### Rule 7 (affix ی)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0e312945",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def rule_7(wrd, pos=None, pos_last=None):\n",
    "    if not 'ی'==wrd[-1]:\n",
    "        return wrd\n",
    "    else:\n",
    "        wrd = m0_9.process_wrd(wrd, pos)\n",
    "        if not pos_last=='Verb':\n",
    "            if wrd[-1]=='i':\n",
    "                wrd = wrd[:-1] + 'ye'\n",
    "        return wrd\n",
    "    \n",
    "tests = [{'farsi': 'بانوی', 'trans': 'bAnuye', 'pos': 'Noun', 'pos_last': 'Adjective'}, \n",
    "         {'farsi': 'چیزی', 'trans': 'Cizi', 'pos': 'Noun', 'pos_last': 'Verb'}]\n",
    "\n",
    "for t in tests:\n",
    "    assert rule_7(t['farsi'], pos=t['pos'],  pos_last=t['pos_last']) == t['trans']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4bd71f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d372ed68",
   "metadata": {},
   "source": [
    "### Rule 8 (letter ۀ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "36649921",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def rule_8(wrd, pos=None):\n",
    "    if not wrd[-1] in ['ٔ', 'ۀ']:\n",
    "        return wrd\n",
    "    else:\n",
    "        wrd = m0_9.process_wrd(wrd[:-1], pos=pos)\n",
    "        if wrd[-1] == 'e':\n",
    "            wrd += 'ye'\n",
    "        else:\n",
    "            wrd += 'eye'\n",
    "        return wrd\n",
    "\n",
    "tests = [{'farsi': 'همۀ', 'trans': 'hameye', 'pos': 'Determiner'}, \n",
    "         {'farsi': 'بچهٔ', 'trans': 'baCCeye', 'pos': 'Verb'}]\n",
    "\n",
    "for t in tests:\n",
    "    # print(rule_8(t['farsi'], pos=t['pos']))\n",
    "    assert rule_8(t['farsi'], pos=t['pos']) == t['trans']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17662182",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2d9e191a",
   "metadata": {},
   "source": [
    "### Rule 9 (collision affix ات)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "88522bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def rule_9(wrd, pos=None):\n",
    "    if not 'ات' in wrd:\n",
    "        return wrd\n",
    "    else:\n",
    "        if 'ات‌' in wrd:\n",
    "            d_affixes = m0_9.get_affixes(wrd, 'ات‌') #, stem)\n",
    "            stem = 'At'\n",
    "        elif '\\u200cات' in wrd:\n",
    "            d_affixes = m0_9.get_affixes(wrd, '\\u200cات')\n",
    "            stem = '\\'at'\n",
    "        else:\n",
    "            d_affixes = m0_9.get_affixes(wrd, 'ات')\n",
    "            stem = 'At'\n",
    "\n",
    "    if len(d_affixes['prefix']) > len(d_affixes['suffix']):\n",
    "        return m0_9.general_search(d_affixes['prefix'], pos_pos=pos) + stem + m0_9.affix_search(d_affixes['suffix'] )\n",
    "    else:\n",
    "        return m0_9.affix_search(d_affixes['prefix']) + stem + m0_9.general_search(d_affixes['suffix'], pos_pos=pos)\n",
    "\n",
    "\n",
    "tests = [{'farsi': 'مزخرفات', 'trans': 'mozaxrafAt'},\n",
    "         {'farsi': 'خاطرات‌مان', 'trans': 'xAterAtemAn'},\n",
    "         {'farsi': 'عمه‌ات', 'trans': \"'amme'at\"}]\n",
    "\n",
    "\n",
    "for t in tests:\n",
    "    assert rule_9(m0_9.normalise(t['farsi'])) == t['trans']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba31ed3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "70be2230",
   "metadata": {},
   "source": [
    "### Rule 10 (collision affix ان)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a56dd1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def rule_10(wrd, pos=None):\n",
    "    if not 'ان' in wrd:\n",
    "        return wrd\n",
    "    else:\n",
    "        if len(wrd) == 2:\n",
    "            return 'An'\n",
    "        elif wrd[3] == 'ی':\n",
    "            return m0_9.general_search(wrd[:-2], pos_pos=pos) + 'yAn'\n",
    "        else: \n",
    "            return m0_9.general_search(wrd[:-2], pos_pos=pos) + 'An'\n",
    "\n",
    "        \n",
    "tests = [{'farsi': 'بانیان', 'trans': 'bAniyAn', 'pos': 'Noun'},\n",
    "         {'farsi': 'دیگران', 'trans': 'digarAn', 'pos': 'Preposition'}]\n",
    "\n",
    "\n",
    "for t in tests:\n",
    "    assert rule_10(m0_9.normalise(t['farsi'])) == t['trans']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff37011c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9293868f",
   "metadata": {},
   "source": [
    "### Rule 11 (collision affix ش)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a506bd12",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def rule_11(wrd, pos=None):\n",
    "    if 'ش' != wrd[-1]:\n",
    "        return wrd\n",
    "    else:\n",
    "        if pos == 'Noun':\n",
    "            return m0_9.process_noun(wrd[:-1])+'aS'\n",
    "        elif pos == 'Verb':\n",
    "            return m0_9.process_verb(wrd[:-1])+'eS'\n",
    "        else:\n",
    "            return m0_9.general_search(wrd[:-1], pos_pos=pos)+'eS'\n",
    "\n",
    "tests = [{'farsi': 'وسایلش', 'trans':'vasAyelaS', 'pos': 'Noun'},\n",
    "         {'farsi': 'بردندش', 'trans': 'bordandeS', 'pos': 'Verb'}]\n",
    "\n",
    "for t in tests:\n",
    "    assert rule_11(t['farsi'], t['pos']) == t['trans']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8b787e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0782d384",
   "metadata": {},
   "source": [
    "### Rule 12 (affix م)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a97af138",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tests = [{'farsi': 'چندم', 'trans':'Candom', 'pos': 'Number'}]\n",
    "\n",
    "for t in tests:\n",
    "    assert m0_9.process_wrd(t['farsi'], t['pos']) == t['trans']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d31425",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8cbf34f6",
   "metadata": {},
   "source": [
    "### Rule 13 (affix مان)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3c94c534",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def rule_13(wrd, pos=None):\n",
    "    if len(wrd) > 3:\n",
    "        if wrd[-4:] == '\\u200cمان':\n",
    "            wrd = m0_9.process_wrd(wrd[:-4], pos)\n",
    "            wrd += 'mAn' if wrd[-1] in ['a','e','o','A','i','u'] else 'emAn'\n",
    "            return wrd\n",
    "    return wrd\n",
    "\n",
    "tests = [{'farsi': 'برای‌مان', 'trans':'barAyemAn', 'pos': 'Preposition'},\n",
    "         {'farsi': 'خاطرات‌مان', 'trans':'xAterAtemAn', 'pos': 'Noun'},\n",
    "         # {'farsi': 'کردن‌مان', 'trans': 'kardanemAn', 'pos': 'Noun'}\n",
    "        ]\n",
    "\n",
    "for t in tests:\n",
    "    # print(rule_13(t['farsi'], t['pos']))\n",
    "    assert rule_13(t['farsi'], t['pos']) == t['trans']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23e515b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6d78c1a5",
   "metadata": {},
   "source": [
    "### Rule 14 (affix می)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bd037b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def rule_14(wrd, pos=None):\n",
    "    if not 'می' in wrd:\n",
    "        return wrd\n",
    "    else:\n",
    "        if 'می‌' == wrd[:3]:\n",
    "            return 'mi'+m0_9.process_wrd(wrd[3:], pos)\n",
    "        elif 'می' == wrd[-2:]:\n",
    "            return m0_9.general_search(wrd[:-2]) + 'omi'\n",
    "        \n",
    "tests = [{'farsi': 'می‌تواند', 'trans': 'mitavAnad', 'pos': 'Verb'},\n",
    "         {'farsi': 'چندمی', 'trans': 'Candomi'}#, \n",
    "         #{'farsi': 'sdd', 'trans': 'Amjsnkjs'}\n",
    "        ]\n",
    "\n",
    "for t in tests:\n",
    "    assert rule_14(t['farsi'], t.get('pos', None)) == t['trans']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224bde0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "507bfbf3",
   "metadata": {},
   "source": [
    "### Rule 15 (affix آ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2ef8d4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "text = 'بیا در آغوشم بیارام و دیگران را نیازار'\n",
    "# 'biyA dar 'AquSam biyArAm va digarAn rA nayAzAr'\n",
    "#assets.tagger.tag(assets.word_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab60022",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "32c9809b",
   "metadata": {},
   "source": [
    "### Rule 16 (affix ون)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "76d7fc3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def rule_16(wrd, pos=None):\n",
    "    if not 'ون' == wrd[-2:]:\n",
    "        return wrd\n",
    "    else:\n",
    "        w = m0_9.process_wrd(wrd, pos) \n",
    "        # print('dbg: ', w)\n",
    "        if w != wrd:\n",
    "            return w\n",
    "        else:\n",
    "            w = m0_9.process_wrd(wrd[:-2], pos)\n",
    "            w += 'yun' if w[-1] == 'i' else 'un'\n",
    "            return w\n",
    "        \n",
    "tests = [{'farsi': 'سرنگون', 'trans': 'sarnegun', 'pos': 'Adjective'},\n",
    "         {'farsi': 'حواریون', 'trans': 'havAriyun', 'pos': 'Verb'},\n",
    "         {'farsi': 'منافقون', 'trans': 'monAfequn', 'pos': 'Noun'}]\n",
    "\n",
    "\n",
    "for t in tests:\n",
    "    # print(rule_16(t['farsi'], t.get('pos', None)))\n",
    "    assert rule_16(t['farsi'], t.get('pos', None)) == t['trans']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3ff345",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "49736a42",
   "metadata": {},
   "source": [
    "### Rule 17 (affix ید)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9185dd98",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def rule_17(wrd, pos=None):\n",
    "    if not 'ید' in wrd:\n",
    "        return wrd\n",
    "    elif pos == 'Verb':\n",
    "        l_lemma = [w for w in assets.lemmatizer.lemmatize(wrd).split('#') if w == wrd[:len(w)]]\n",
    "        if len(l_lemma) > 0:\n",
    "            # print(1)\n",
    "            return m0_9.process_verb(wrd[:-len('ید')]) + 'id'\n",
    "        else:\n",
    "            return m0_9.process_verb(wrd[:-len('ید')])+'yad'\n",
    "    else:\n",
    "        return wrd # ???\n",
    "    \n",
    "tests = [{'farsi': 'رفتید', 'trans': 'raftid', 'pos': 'Verb'},\n",
    "         {'farsi': 'بگوید', 'trans': 'beguyad', 'pos': 'Verb'},\n",
    "         #{'farsi': 'می‌آید', 'trans': 'beguyad', 'pos': 'Verb'}\n",
    "        ]\n",
    "\n",
    "for t in tests:\n",
    "    assert rule_17(t['farsi'], t.get('pos', None)) == t['trans']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478a90a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "42b50edb",
   "metadata": {},
   "source": [
    "### Rule 18 (verb with یم)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c62a7ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tests = [{'farsi': 'بپریم', 'trans': 'beparim', 'pos': 'Verb'},\n",
    "         {'farsi': 'رفتیم', 'trans': 'raftim', 'pos': 'Verb'},\n",
    "         {'farsi': 'خدایم', 'trans': 'xodAyam', 'pos': 'Noun'}]\n",
    "\n",
    "for t in tests:\n",
    "    assert m0_9.process_wrd(t['farsi'], t.get('pos', None)) == t['trans']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a90b5ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "50bb7e78",
   "metadata": {},
   "source": [
    "### Rule 19 (Semispace u200c, improve imlementation in 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "65935ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def rule_19(wrd, pos):\n",
    "    if not '\\u200c' in wrd:\n",
    "        return wrd\n",
    "    else:\n",
    "        l_wrd = wrd.split('\\u200c')\n",
    "        M = max([len(w) for w in l_wrd])\n",
    "        _str = []\n",
    "        for i in range(len(l_wrd)):\n",
    "            w = l_wrd[i]\n",
    "            if len(w) == M:\n",
    "                _str.append(m0_9.process_wrd(w, pos))\n",
    "            else:\n",
    "                w_tmp = m0_9.process_verb(w)\n",
    "                if w != w_tmp:\n",
    "                    _str.append(w_tmp)\n",
    "                else:\n",
    "                    _str.append(recu_affixes(w))\n",
    "    return ''.join(_str)\n",
    "                    \n",
    "tests = [{'farsi': 'دیده_می\\u200cشود', 'trans': 'dide miSavad', 'pos': 'Verb'},\n",
    "         {'farsi': 'بی\\u200cبصیرتی\\u200cهایی', 'trans': 'bibasiratihAyi', 'pos': 'Noun'},\n",
    "         {'farsi': 'چراغ‌علی', 'trans': \"CerAq'ali\", 'pos': 'Noun'}]\n",
    "\n",
    "for t in tests:\n",
    "    #print(rule_19(t['farsi'], t['pos']))\n",
    "    assert rule_19(t['farsi'], t['pos']) == t['trans']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6626eb1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "61a6fee1",
   "metadata": {},
   "source": [
    "### Rule 20 (Recursive search, improve implemented in 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "id": "2b5eca86",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def recu_affix_wrd(wrd):\n",
    "    \n",
    "    suffix, stem = '', ''\n",
    "    for i in range(len(wrd), 0, -1):\n",
    "        if assets.df_Affixes[assets.df_Affixes['Affix']==wrd[:i]].shape[0] > 0:\n",
    "            l_search = assets.df_Affixes[assets.df_Affixes['Affix']==wrd[:i]].to_dict('records')\n",
    "            stem = m0_9.votation_entries(l_search, entries=False)\n",
    "            suffix = recu_affix_wrd(wrd[i:])\n",
    "            break\n",
    "\n",
    "    return stem + suffix\n",
    "\n",
    "tests = [{'farsi': 'هایی', 'trans': 'hAyi'},\n",
    "         {'farsi': 'هایتان', 'trans': 'hAyetAn'}]\n",
    "\n",
    "for t in tests:\n",
    "    assert recu_affix_wrd(t['farsi']) == t['trans']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd673775",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a87eb4b9",
   "metadata": {},
   "source": [
    "### Rule 21 (affix ن)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "aa669874",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def rule_21(wrd, pos=None):\n",
    "    if not (wrd[-1] == 'ن' or wrd[0] == 'ن'):\n",
    "        return m0_9.process_wrd(wrd, pos)\n",
    "    else:\n",
    "        if wrd[-1] == 'ن':\n",
    "            return rule_21(wrd[:-1], pos='Verb') + 'an'\n",
    "        elif wrd[0] == 'ن':\n",
    "            return 'na' + rule_21(wrd[1:], pos='Verb')\n",
    "\n",
    "\n",
    "tests = [{'farsi': 'نخوردن', 'trans': 'naxordan'},\n",
    "         {'farsi': 'نخوابیدن', 'trans': 'naxAbidan'},\n",
    "         #{'farsi': 'نمان', 'trans': 'namAn'}\n",
    "         ]\n",
    "\n",
    "for t in tests:\n",
    "    #print(rule_21(t['farsi']))\n",
    "    assert rule_21(t['farsi']) == t['trans']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08867f86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "45dda2dd",
   "metadata": {},
   "source": [
    "### Rule 22 (affixes بی and نی)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6c733150",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def rule_22(wrd, pos=None):\n",
    "    if not wrd[:2] in ['بی', 'نی']:\n",
    "        return wrd\n",
    "    else:\n",
    "        suffix = ''.join([s for s in recu_affixes(wrd[2:])\n",
    "                          if s!='\\''])\n",
    "        return m0_9.affix_search(wrd[:2]) + suffix\n",
    "\n",
    "\n",
    "tests = [{'farsi': 'بیا', 'trans': 'biyA'},\n",
    "         #{'farsi': 'نیازار', 'trans': 'nayAzAr'},\n",
    "         #{'farsi': 'نمان', 'trans': 'namAn'}\n",
    "         #{'farsi': 'بیارام', 'trans': 'biyArAm'}\n",
    "         ]\n",
    "\n",
    "for t in tests:\n",
    "    # print(rule_22(t['farsi']))\n",
    "    assert rule_22(t['farsi']) == t['trans']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f7fd5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2717ce3e",
   "metadata": {},
   "source": [
    "### Rule 23 (root of the verb is رو)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1b393098",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def rule_23(wrd, pos=None):\n",
    "    if not wrd[-2:] == 'رو':\n",
    "        return wrd\n",
    "    else:\n",
    "        l_lemma = assets.lemmatizer.lemmatize(wrd).split('#')\n",
    "        if len(l_lemma) > 1:\n",
    "            lemma = [l for l in l_lemma if l=='رو'][0]\n",
    "            d_affixes = m0_9.get_affixes(wrd, lemma)\n",
    "            prefix = d_affixes['prefix']\n",
    "            return m0_9.affix_search(wrd[:-2]) + 'ro'\n",
    "        else:\n",
    "            d_affixes = m0_9.get_affixes(wrd, 'رو')\n",
    "            prefix = d_affixes['prefix']\n",
    "            return m0_9.affix_search(wrd[:-2]) + 'ro'\n",
    "    \n",
    "tests = [{'farsi': 'نرو', 'trans': 'naro'},\n",
    "         {'farsi': 'برو', 'trans': 'bero'}]\n",
    "\n",
    "for t in tests:\n",
    "    assert rule_23(t['farsi']) == t['trans']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c766e27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b928ba2a",
   "metadata": {},
   "source": [
    "### Rule 24 (Recursion, improve v0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3755019c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def recu_entries(wrd):\n",
    "    \n",
    "    for i in range(len(wrd), 0, -1):\n",
    "        if assets.df_Entries[assets.df_Entries['WrittenForm']==wrd[:i]].shape[0] > 0:\n",
    "            l_search = assets.df_Entries[assets.df_Entries['WrittenForm']==wrd[:i]].to_dict('records')\n",
    "            return m0_9.votation_entries(l_search) + recu_entries(wrd[i:])\n",
    "            break\n",
    "\n",
    "    return wrd\n",
    "\n",
    "\n",
    "def recu_affixes(wrd):\n",
    "    for i in range(len(wrd), 0, -1):\n",
    "        if assets.df_Affixes[assets.df_Affixes['Affix']==wrd[:i]].shape[0] > 0:\n",
    "            l_search = assets.df_Affixes[assets.df_Affixes['Affix']==wrd[:i]].to_dict('records')\n",
    "            return m0_9.votation_entries(l_search, entries=False) + recu_affixes(wrd[i:])\n",
    "            break\n",
    "\n",
    "    return wrd\n",
    "\n",
    "\n",
    "tests = [{'farsi': 'فیسبوک', 'trans': 'fisbuke'}]\n",
    "\n",
    "for t in tests:\n",
    "    assert recu_entries(t['farsi']) == t['trans']\n",
    "\n",
    "tests = [{'farsi': 'ازار', 'trans': 'AzAr'}]\n",
    "\n",
    "for t in tests:\n",
    "    assert recu_affixes(t['farsi']) == t['trans']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79aa0bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b4a4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = 'دستشان در دستتان و دستم در دستت! واتساپ و فیسبوک با ما چه کرده‌اند؟ دست بزنید!'\n",
    "# 'dasteSAn dar dastetAn va dastam dar dastat! vAtsAp va fisbuk bA mA Ce karde'and? dast bezanid!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d361406",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
